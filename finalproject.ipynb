{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFQpbVtW7kVM"
   },
   "source": [
    "# Final project - MovieLens recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "S_t73_iG7kVO",
    "outputId": "feb1e96b-0232-4504-e9ea-a127fca9e492"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import lightfm\n",
    "from lightfm import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xxXi3QxX7kVW",
    "outputId": "6800d516-5b60-45f8-ce30-f35ba9300dd6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1        2     3.5  1112486027\n",
       "1       1       29     3.5  1112484676\n",
       "2       1       32     3.5  1112484819\n",
       "3       1       47     3.5  1112484727\n",
       "4       1       50     3.5  1112484580"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = pd.read_csv(('./ml-20m/ratings.csv'), header=0)\n",
    "original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the original data, there are 20000263 rating events from 138493 users and 26744 movies (sparsity: 0.540%)\n"
     ]
    }
   ],
   "source": [
    "n_users = original.userId.unique().shape[0]\n",
    "n_items = original.movieId.unique().shape[0]\n",
    "sparsity = float(original.shape[0]) / float(n_users*n_items) * 100\n",
    "\n",
    "print(\"In the original data, there are %d rating events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (original.shape[0], n_users, n_items, sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXr877eruDNZ"
   },
   "source": [
    "# FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkyowecRuGfu"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uoHL4p7Jm1-h",
    "outputId": "0975b3e2-6716-4e77-e766-f2731e2d00e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, there are 20000263 rating events from 138493 users and 26744 movies (sparsity: 0.540%)\n"
     ]
    }
   ],
   "source": [
    "#dataset = original.sample(frac=0.1, replace=True)   #if you want to randomly sample the original data, uncomment this line\n",
    "dataset = original  \n",
    "n_users = dataset.userId.unique().shape[0]\n",
    "n_items = dataset.movieId.unique().shape[0]\n",
    "sparsity = float(dataset.shape[0]) / float(n_users*n_items) * 100\n",
    "print(\"Before filtering, there are %d rating events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (dataset.shape[0], n_users, n_items, sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXUGT2Aw7kVg"
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "def filter_triplets(tp, min_uc=0, min_sc=0):\n",
    "    # Only keep the triplets for items which were rated by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who rated at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bp2seUkyvOi5"
   },
   "source": [
    "Only keep items that are rated by at least 50 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0pBZvKarxGX"
   },
   "outputs": [],
   "source": [
    "dataset, user_fm, item_fm = filter_triplets(dataset,min_sc = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qQCr9EtKs_Qo",
    "outputId": "9206908a-6854-4bb0-b435-2fd53c6a8cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 19847947 rating events from 138493 users and 10524 movies (sparsity: 1.362%) for lightfm\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * dataset.shape[0] / (user_fm.shape[0] * item_fm.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d rating events from %d users and %d movies (sparsity: %.3f%%) for lightfm\" % \n",
    "      (dataset.shape[0], user_fm.shape[0], item_fm.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "HmUspRlIwqEz",
    "outputId": "d284a4ac-0d6c-4941-f22c-d5aed2a9dbc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1        2     3.5  1112486027\n",
       "1       1       29     3.5  1112484676\n",
       "2       1       32     3.5  1112484819\n",
       "3       1       47     3.5  1112484727\n",
       "4       1       50     3.5  1112484580"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNSr1fDTwWyi"
   },
   "outputs": [],
   "source": [
    "def sparse_matrix(dataset):\n",
    "    movieId_to_idx = {}\n",
    "    idx_to_movieId = {}\n",
    "    for (idx, movieId) in enumerate(dataset.movieId.unique().tolist()):\n",
    "        movieId_to_idx[movieId] = idx\n",
    "        idx_to_movieId[idx] = movieId\n",
    "    \n",
    "    userId_to_idx = {}\n",
    "    idx_to_userId = {}\n",
    "    for (idx, userId) in enumerate(dataset.userId.unique().tolist()):\n",
    "        userId_to_idx[userId] = idx\n",
    "        idx_to_userId[idx] = userId\n",
    "    \n",
    "    def map_ids(row, mapper):\n",
    "        return mapper[row]\n",
    "    \n",
    "    I = dataset.userId.apply(map_ids, args=[userId_to_idx]).values\n",
    "    J = dataset.movieId.apply(map_ids, args=[movieId_to_idx]).values\n",
    "    V = np.ones(I.shape[0])\n",
    "    data = scipy.sparse.coo_matrix((V, (I, J)), dtype=np.float64)\n",
    "    data = data.tocsr()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjiVM7VhzHHj"
   },
   "outputs": [],
   "source": [
    "data = sparse_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZH4XSTwzb53"
   },
   "source": [
    "### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t52ELREgzNFh"
   },
   "outputs": [],
   "source": [
    "train, test = lightfm.cross_validation.random_train_test_split(data,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_N6w-PFzPRr"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc6ycxjFev5e"
   },
   "source": [
    "### evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Hnp4HemmdUk"
   },
   "outputs": [],
   "source": [
    "def ndcgatk(x_test, x_predict, k):\n",
    "    ndcg_values = []\n",
    "    total_ndcg = 0.0\n",
    "    best  = 0.0\n",
    "    for i in range(len(x_test)):\n",
    "        top_rated_movies_idx = [i for i, x in enumerate(x_test[i].tolist()) if x == 1.0]\n",
    "\n",
    "        if len(top_rated_movies_idx) == 0:\n",
    "            #print(\"test user has no 1 rated movies: \", i)\n",
    "            continue\n",
    "        sorted_ratings = x_predict[i].tolist()\n",
    "        top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "        sum_ndcg = 0\n",
    "        for i in range(0, k):\n",
    "            if top_predicted_movies_idx[i] in top_rated_movies_idx:\n",
    "                ndcg = 1/(math.log(i+2))\n",
    "            else:\n",
    "                ndcg = 0\n",
    "            sum_ndcg += ndcg\n",
    "\n",
    "        total_ndcg += sum_ndcg\n",
    "        ndcg_values.append(sum_ndcg)\n",
    "\n",
    "    ndcg_values = np.array(ndcg_values)\n",
    "    max_ndcg = ndcg_values.max()\n",
    "    ndcg_values = ndcg_values / max_ndcg \n",
    "    total_ndcg = np.sum(ndcg_values)\n",
    "\n",
    "    return total_ndcg/float(len(ndcg_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMib7Ag4fgD6"
   },
   "outputs": [],
   "source": [
    "def recallatk(x_test, x_predict, k):\n",
    "    recall_values = []\n",
    "    total_recall = 0.0\n",
    "    for i in range(len(x_test)):\n",
    "        top_rated_movies_idx = [i for i, x in enumerate(x_test[i].tolist()) if x == 1.0]\n",
    "\n",
    "        if len(top_rated_movies_idx) == 0:\n",
    "            #print(\"test user has no 1 rated movies: \", i)\n",
    "            continue\n",
    "\n",
    "        sorted_ratings = x_predict[i].tolist()\n",
    "        top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "\n",
    "        sum = 0.0\n",
    "        for i in range(0, k):\n",
    "            if top_predicted_movies_idx[i] in top_rated_movies_idx:\n",
    "                sum+=1.0\n",
    "        recall = sum/float(min(k, len(top_rated_movies_idx)))\n",
    "        total_recall += recall\n",
    "        recall_values.append(recall)\n",
    "    return total_recall/float(len(recall_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wI22IZeFziVF"
   },
   "source": [
    "### without side information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhjzmI0hzXJ6"
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DKuXXOzPadeE",
    "outputId": "010261b8-e396-424e-aecc-837cc30c4871"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x214cdc5ab38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only use one epoch because of the running time\n",
    "model = LightFM(learning_rate=0.05,loss = 'warp')\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "predict_rank = model.predict_rank(test)\n",
    "predict_rank = predict_rank.toarray()\n",
    "predict_rank[predict_rank == 0] = inf\n",
    "predict_ranks = pd.DataFrame(data = predict_rank, index= list(range(0,predict_rank.shape[0])),columns= list(range(0,predict_rank.shape[1])))\n",
    "predict_ranks = predict_ranks.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogcoverage(k,predict_ranks):\n",
    "    item_count = []\n",
    "    for uid in list(range(0,predict_ranks.shape[1])):\n",
    "        recs = predict_ranks.iloc[:,uid:uid+1].nsmallest(k,uid).index\n",
    "        recs = recs.tolist()\n",
    "        item_count.append(recs)\n",
    "    n_items = predict_ranks.shape[0]\n",
    "    item_count_flat = []\n",
    "    for sublist in item_count:\n",
    "        for item in sublist:\n",
    "            item_count_flat.append(item)\n",
    "    item_count = list(set(item_count_flat))\n",
    "    cc = len(item_count)/n_items\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = list(range(0,train.shape[0]))\n",
    "mid = list(range(0,train.shape[1]))\n",
    "predict_train = []\n",
    "for i in uid:\n",
    "    predict = model.predict(i, mid)\n",
    "    predict_train.append(predict.tolist())\n",
    "predict_train = np.array(predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = []\n",
    "for i in uid:\n",
    "    predict = model.predict(i, mid)\n",
    "    predict_test.append(predict.tolist())\n",
    "predict_test = np.array(predict_test)\n",
    "\n",
    "x_train = train.toarray()\n",
    "x_test = test.toarray()\n",
    "recall_train_20 = recallatk(x_train,predict_train,20)\n",
    "recall_train_50 = recallatk(x_train,predict_train,50)\n",
    "recall_test_20 = recallatk(x_test,predict_test,20)\n",
    "recall_test_50 = recallatk(x_test,predict_test,50)\n",
    "ndcg_train = ndcgatk(x_train, predict_train,100)\n",
    "ndcg_test = ndcgatk(x_test, predict_test,100)\n",
    "cc_20 = catalogcoverage(20,predict_ranks)\n",
    "cc_50 = catalogcoverage(50,predict_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xmtjS5KzoNB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@20: train 0.33, test 0.12.\n",
      "Recall@50: train 0.31, test 0.17.\n",
      "NDCG@100: train 0.25, test 0.19.\n",
      "CatalogCoverage@20: 0.18.\n",
      "CatalogCoverage@50: 0.28.\n"
     ]
    }
   ],
   "source": [
    "print('Recall@20: train %.2f, test %.2f.' % (recall_train_20, recall_test_20))\n",
    "print('Recall@50: train %.2f, test %.2f.' % (recall_train_50, recall_test_50))\n",
    "print('NDCG@100: train %.2f, test %.2f.' % (ndcg_train, ndcg_test))\n",
    "print('CatalogCoverage@20: %.2f.' % (cc_20))\n",
    "print('CatalogCoverage@50: %.2f.' % (cc_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise.prediction_algorithms.knns import KNNBaseline\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection.search import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Dataset\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(line_format='user item rating', sep=',', skip_lines=3, rating_scale=(1, 5))\n",
    "cf_dataset=pd.read_csv('../ml-latest-small/ratings.csv')\n",
    "cf_dataset=cf_dataset.drop(['timestamp'],axis=1)\n",
    "cf_data, user_fm, item_fm = filter_triplets(cf_dataset,min_sc = 50)\n",
    "cf_data=Dataset.load_from_df(cf_data,reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBaseline at 0x2c5a8d53b38>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainset, testset = train_test_split(cf_data, test_size=.2)\n",
    "algo = KNNBaseline(k=80)\n",
    "algo.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def surprise_recall_at_k(predictions, k=10, threshold=4):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "def catalogcoverage(top_n):\n",
    "    item_count = []\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        top_k = [iid for (iid, _) in user_ratings]\n",
    "        item_count.append(top_k)\n",
    "    item_count = list(itertools.chain(*item_count))\n",
    "    item_count = list(set(item_count))\n",
    "    n_items = cf_dataset.movieId.unique().shape[0]\n",
    "    cc = len(item_count)/n_items\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls_test_20 = surprise_recall_at_k(predict_test,20)\n",
    "recall_test_20 = sum(rec for rec in recalls_test_20.values()) / len(recalls_test_20)\n",
    "recalls_test_50 = surprise_recall_at_k(predict_test,50)\n",
    "recall_test_50 = sum(rec for rec in recalls_test_50.values()) / len(recalls_test_50)\n",
    "catalogcoverage_20 = catalogcoverage(get_top_n(predict_test,20))\n",
    "catalogcoverage_50 = catalogcoverage(get_top_n(predict_test,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@20: test 0.48.\n",
      "Recall@50: test 0.49.\n",
      "Catalog coverage@20: test 0.05.\n",
      "Catalog coverage@50: test 0.05.\n"
     ]
    }
   ],
   "source": [
    "#recall_test = recallatk(testset,predict_test,20)\n",
    "#ndcg_test = ndcgatk(testset, predict_test,100)\n",
    "print('Recall@20: test %.2f.' % (recall_test_20))\n",
    "print('Recall@50: test %.2f.' % (recall_test_50))\n",
    "print ('Catalog coverage@20: test %.2f.' % (catalogcoverage_20))\n",
    "print ('Catalog coverage@50: test %.2f.' % (catalogcoverage_50))\n",
    "# print('NDCG@100: train %.2f, test %.2f.' % (ndcg_train, ndcg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVDpp\n",
    "algo = SVDpp(lr_all = 0.011, reg_all = 0.1)\n",
    "algo.fit(trainset)\n",
    "predict_test = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@20: test 0.47.\n",
      "Recall@50: test 0.47.\n",
      "Catalog coverage@20: test 0.05.\n",
      "Catalog coverage@50: test 0.05.\n"
     ]
    }
   ],
   "source": [
    "recalls_test_20 = surprise_recall_at_k(predict_test,20)\n",
    "recall_test_20 = sum(rec for rec in recalls_test_20.values()) / len(recalls_test_20)\n",
    "recalls_test_50 = surprise_recall_at_k(predict_test,50)\n",
    "recall_test_50 = sum(rec for rec in recalls_test_50.values()) / len(recalls_test_50)\n",
    "catalogcoverage_20 = catalogcoverage(get_top_n(predict_test,20))\n",
    "catalogcoverage_50 = catalogcoverage(get_top_n(predict_test,50))\n",
    "print('Recall@20: test %.2f.' % (recall_test_20))\n",
    "print('Recall@50: test %.2f.' % (recall_test_50))\n",
    "print ('Catalog coverage@20: test %.2f.' % (catalogcoverage_20))\n",
    "print ('Catalog coverage@50: test %.2f.' % (catalogcoverage_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AzWgkJyuAFl"
   },
   "source": [
    "# VAE with Genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdoznVhe7kVR"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKjs_KvdnDg7"
   },
   "source": [
    "We need to binarize the data for vae.                     \n",
    "We only keep ratings >= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LywqzdJ7kVY"
   },
   "outputs": [],
   "source": [
    "# binarize the data (only keep ratings >= 4)\n",
    "raw_data = original[original['rating'] > 3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "4fSHRfRa7kVb",
    "outputId": "29f541ee-d3cc-4441-95cf-9ec79cfc0afb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1094785734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112485573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1112484703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId  rating   timestamp\n",
       "6        1      151     4.0  1094785734\n",
       "7        1      223     4.0  1112485573\n",
       "8        1      253     4.0  1112484940\n",
       "9        1      260     4.0  1112484826\n",
       "10       1      293     4.0  1112484703"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBd0elm67kVf"
   },
   "source": [
    "### Data splitting procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQtBMcDB7kVf"
   },
   "source": [
    "- Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
    "- Use all the items from the training users as item set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PiPENPb1zDv-",
    "outputId": "0dacde19-17b3-418d-d897-9473a0dc5db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, there are 9995410 rating events from 138287 users and 20720 movies (sparsity: 0.349%)\n"
     ]
    }
   ],
   "source": [
    "n_users = raw_data.userId.unique().shape[0]\n",
    "n_items = raw_data.movieId.unique().shape[0]\n",
    "sparsity = float(raw_data.shape[0]) / float(n_users*n_items) * 100\n",
    "\n",
    "print(\"Before filtering, there are %d rating events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], n_users, n_items, sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8xpdYjQ7kVk"
   },
   "source": [
    "Only keep items that are rated by at least 50 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "844467Jd7kVl"
   },
   "outputs": [],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data,min_sc=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0ojiptXz7kVn",
    "outputId": "2e32dcad-b883-4994-85c8-25129b6867aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9868061 rating events from 138287 users and 7345 movies (sparsity: 0.972%) for VAE\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d rating events from %d users and %d movies (sparsity: %.3f%%) for VAE\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OxTYDcG7kVp"
   },
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8F4k5PYa7kVr"
   },
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAJDJagb7kVt"
   },
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvrhpaGj7kVv"
   },
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgOt6Xwq7kVx"
   },
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PjxEYhE7kV1"
   },
   "outputs": [],
   "source": [
    "with open(('unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGSOWiE-7kV5"
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmNp-YJs7kV8"
   },
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "3zQUdCyA7kV-",
    "outputId": "731a617e-1c17-427f-a016-1a4f3a67c811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5QuPSeX7kWB"
   },
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "XHcwxeUt7kWD",
    "outputId": "449bd8a5-30cf-41a0-f25b-b0ecf9f8a3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZnwu-a87kWF"
   },
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFRmKV0g7kWF"
   },
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45WjoLMl7kWH"
   },
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(('train.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76zoaA5M7kWJ"
   },
   "outputs": [],
   "source": [
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(('validation_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwDHzuEe7kWL"
   },
   "outputs": [],
   "source": [
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(('validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1itrh7wd7kWN"
   },
   "outputs": [],
   "source": [
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(('test_tr.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNWamdLw7kWO"
   },
   "outputs": [],
   "source": [
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(('test_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTyDe8dydbWa"
   },
   "source": [
    "### get item-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UQTjRhedyG0"
   },
   "source": [
    "we binarize the genomes by seeting the top 20 genomes of each movie to 1, and the others to 0.\n",
    "For each movie, we record the top 20 genomes's id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jX_0oQL65OvL"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "NUM_GENOMES = 1128\n",
    "num_genomes = 1128\n",
    "\n",
    "#converts tuple to a 1128 dim vector\n",
    "def get_genome_vec(genome_tup):\n",
    "    vec = np.zeros(NUM_GENOMES)\n",
    "    #print len(genome_tup)\n",
    "    for i in genome_tup:\n",
    "        tag = int(i[0])-1\n",
    "        vec[tag] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "with open('./ml-20m/genome-scores.csv', 'r') as f:\n",
    "    genome_data = f.read().splitlines()\n",
    "\n",
    "genome_dict = {}\n",
    "\n",
    "#Collect all the genomes in genomes-scores.csv\n",
    "for i in genome_data[1:]:\n",
    "    i = i.split(\",\")\n",
    "    mid = i[0]\n",
    "    tagid = i[1]\n",
    "    relevance = float(i[2])\n",
    "    try :\n",
    "        genome_dict[mid].append((tagid, relevance))\n",
    "    except:\n",
    "        genome_dict[mid] = [(tagid, relevance)]\n",
    "        \n",
    "\n",
    "#sort and select genomes\n",
    "for mid in genome_dict.keys():\n",
    "    scores = genome_dict[mid]\n",
    "    scores = sorted(scores , key=itemgetter(1), reverse = True)\n",
    "    scores = scores[:20]\n",
    "    genome_dict[mid] = scores\n",
    "    #print len(genome_dict[mid])\n",
    "\n",
    "with open('genome_scores.json', 'w') as f:\n",
    "    json.dump(genome_dict, f)\n",
    "\n",
    "unk = np.array([0]*num_genomes)\n",
    "movie_embeddings_array = []\n",
    "\n",
    "#conv_idert list to a one hot vector\n",
    "movie_id = raw_data['movieId'].unique()\n",
    "\n",
    "for i in movie_id:\n",
    "    try:\n",
    "        movie_embeddings_array.append(get_genome_vec(genome_dict[mid]))\n",
    "    except KeyError:\n",
    "        movie_embeddings_array.append(unk)\n",
    "\n",
    "movie_embeddings_array = np.array(movie_embeddings_array)\n",
    "with open('movie_genomes.npy', 'wb') as f:\n",
    "    np.save(f, movie_embeddings_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jP8KNEoA7kWQ"
   },
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1H4a2TC7kWS"
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynnANXHF7kWS"
   },
   "source": [
    "__Notations__: We use $u \\in \\{1,\\dots,U\\}$ to index users and $i \\in \\{1,\\dots,I\\}$ to index items. In this work, we consider learning with implicit feedback. The user-by-item interaction matrix is the click matrix $\\mathbf{X} \\in \\mathbb{N}^{U\\times I}$. The lower case $\\mathbf{x}_u =[X_{u1},\\dots,X_{uI}]^\\top \\in \\mathbb{N}^I$ is a bag-of-words vector with the number of clicks for each item from user u. We binarize the click matrix. It is straightforward to extend it to general count data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyeEhwbA7kWT"
   },
   "source": [
    "__Generative process__: For each user $u$, the model starts by sampling a $K$-dimensional latent representation $\\mathbf{z}_u$ from a standard Gaussian prior. The latent representation $\\mathbf{z}_u$ is transformed via a non-linear function $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ to produce a probability distribution over $I$ items $\\pi (\\mathbf{z}_u)$ from which the click history $\\mathbf{x}_u$ is assumed to have been drawn:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_u \\sim \\mathcal{N}(0, \\mathbf{I}_K),  \\pi(\\mathbf{z}_u) \\propto \\exp\\{f_\\theta (\\mathbf{z}_u\\},\\\\\n",
    "\\mathbf{x}_u \\sim \\mathrm{Mult}(N_u, \\pi(\\mathbf{z}_u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lpFrL6dp7kWV"
   },
   "source": [
    "The objective for Multi-DAE for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\log p_\\theta(\\mathbf{x}_u | g_\\phi(\\mathbf{x}_u))\n",
    "$$\n",
    "where $g_\\phi(\\cdot)$ is the non-linear \"encoder\" function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUIYYXoC7kWX"
   },
   "source": [
    "The objective of Multi-VAE^{PR} (evidence lower-bound, or ELBO) for a single user $u$ is:\n",
    "$$\n",
    "\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n",
    "$$\n",
    "where $q_\\phi$ is the approximating variational distribution (inference model). $\\beta$ is the additional annealing parameter that we control. The objective of the entire dataset is the average over all the users. It can be trained almost the same as Multi-DAE, thanks to reparametrization trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda, merge, Embedding, Flatten, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os    \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Genome and create Genome embedding via VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=13\n",
    "original_dim=1128 \n",
    "intermediate_dim=100\n",
    "latent_dim=3\n",
    "nb_epochs=30\n",
    "epsilon_std=1.0\n",
    "\n",
    "x=Input(batch_shape=(batch_size,original_dim))\n",
    "h=Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    _mean,_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return _mean+K.exp(_log_var/2)*epsilon\n",
    "z= Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "h_decoder=Dense(intermediate_dim, activation='relu')\n",
    "x_bar=Dense(original_dim,activation='sigmoid') \n",
    "h_decoded = h_decoder(z)\n",
    "x_decoded = x_bar(h_decoded)\n",
    "\n",
    "vae = Model(x, x_decoded)\n",
    "def vae_loss(x,x_bar):\n",
    "    reconst_loss=original_dim*objectives.binary_crossentropy(x, x_bar)\n",
    "    kl_loss=-0.5*K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return reconst_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "x_train = np.load( open( \"movie_genomes.npy\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Genome embedding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., epochs=30, steps_per_epoch=565)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "565/565 [==============================] - 4s 7ms/step - loss: 55.8329\n",
      "\n",
      "Epoch 00001: saving model to ./mov_genome.hdf5\n",
      "Epoch 2/30\n",
      "565/565 [==============================] - 3s 6ms/step - loss: 4.8341\n",
      "\n",
      "Epoch 00002: saving model to ./mov_genome.hdf5\n",
      "Epoch 3/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 2.6021\n",
      "\n",
      "Epoch 00003: saving model to ./mov_genome.hdf5\n",
      "Epoch 4/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.5892\n",
      "\n",
      "Epoch 00004: saving model to ./mov_genome.hdf5\n",
      "Epoch 5/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1260\n",
      "\n",
      "Epoch 00005: saving model to ./mov_genome.hdf5\n",
      "Epoch 6/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 0.7444\n",
      "\n",
      "Epoch 00006: saving model to ./mov_genome.hdf5\n",
      "Epoch 7/30\n",
      "565/565 [==============================] - 3s 6ms/step - loss: 0.3814\n",
      "\n",
      "Epoch 00007: saving model to ./mov_genome.hdf5\n",
      "Epoch 8/30\n",
      "565/565 [==============================] - 3s 6ms/step - loss: 0.0615\n",
      "\n",
      "Epoch 00008: saving model to ./mov_genome.hdf5\n",
      "Epoch 9/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 0.0095\n",
      "\n",
      "Epoch 00009: saving model to ./mov_genome.hdf5\n",
      "Epoch 10/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 0.0042\n",
      "\n",
      "Epoch 00010: saving model to ./mov_genome.hdf5\n",
      "Epoch 11/30\n",
      "565/565 [==============================] - 3s 6ms/step - loss: 0.0024\n",
      "\n",
      "Epoch 00011: saving model to ./mov_genome.hdf5\n",
      "Epoch 12/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 0.0016\n",
      "\n",
      "Epoch 00012: saving model to ./mov_genome.hdf5\n",
      "Epoch 13/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 0.0011\n",
      "\n",
      "Epoch 00013: saving model to ./mov_genome.hdf5\n",
      "Epoch 14/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 7.6129e-04\n",
      "\n",
      "Epoch 00014: saving model to ./mov_genome.hdf5\n",
      "Epoch 15/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 5.5227e-04\n",
      "\n",
      "Epoch 00015: saving model to ./mov_genome.hdf5\n",
      "Epoch 16/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 4.0610e-04\n",
      "\n",
      "Epoch 00016: saving model to ./mov_genome.hdf5\n",
      "Epoch 17/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 3.1283e-04\n",
      "\n",
      "Epoch 00017: saving model to ./mov_genome.hdf5\n",
      "Epoch 18/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 2.4195e-04\n",
      "\n",
      "Epoch 00018: saving model to ./mov_genome.hdf5\n",
      "Epoch 19/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.9614e-04\n",
      "\n",
      "Epoch 00019: saving model to ./mov_genome.hdf5\n",
      "Epoch 20/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.6198e-04\n",
      "\n",
      "Epoch 00020: saving model to ./mov_genome.hdf5\n",
      "Epoch 21/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.4134e-04\n",
      "\n",
      "Epoch 00021: saving model to ./mov_genome.hdf5\n",
      "Epoch 22/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.2751e-04\n",
      "\n",
      "Epoch 00022: saving model to ./mov_genome.hdf5\n",
      "Epoch 23/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1968e-04\n",
      "\n",
      "Epoch 00023: saving model to ./mov_genome.hdf5\n",
      "Epoch 24/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1617e-04\n",
      "\n",
      "Epoch 00024: saving model to ./mov_genome.hdf5\n",
      "Epoch 25/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1440e-04\n",
      "\n",
      "Epoch 00025: saving model to ./mov_genome.hdf5\n",
      "Epoch 26/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1362e-04\n",
      "\n",
      "Epoch 00026: saving model to ./mov_genome.hdf5\n",
      "Epoch 27/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1334e-04\n",
      "\n",
      "Epoch 00027: saving model to ./mov_genome.hdf5\n",
      "Epoch 28/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1325e-04\n",
      "\n",
      "Epoch 00028: saving model to ./mov_genome.hdf5\n",
      "Epoch 29/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1321e-04\n",
      "\n",
      "Epoch 00029: saving model to ./mov_genome.hdf5\n",
      "Epoch 30/30\n",
      "565/565 [==============================] - 3s 5ms/step - loss: 1.1319e-04\n",
      "\n",
      "Epoch 00030: saving model to ./mov_genome.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c561768978>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_movie = x_train.shape[0]\n",
    "def nn_batch_generator(x, y, batch_size, samples_per_epoch):\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    x =  x[shuffle_index, :]\n",
    "    y =  y[shuffle_index, :]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        x_batch = x[index_batch,:]\n",
    "        y_batch = y[index_batch,:]\n",
    "        counter += 1\n",
    "        yield (np.array(x_batch),np.array(y_batch))\n",
    "        if (counter >= number_of_batches):\n",
    "            counter=0\n",
    "\n",
    "\n",
    "weightsPath = \"./mov_genome.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=weightsPath, verbose=1)\n",
    "\n",
    "vae.fit_generator(nn_batch_generator(x_train, x_train, batch_size, 565), samples_per_epoch=565, nb_epoch=nb_epochs, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get movie embedding from Genome embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=13\n",
    "original_dim=1128 \n",
    "intermediate_dim=100\n",
    "latent_dim=3\n",
    "nb_epochs=30\n",
    "epsilon_std=1.0\n",
    "\n",
    "x=Input(batch_shape=(batch_size,original_dim))\n",
    "h=Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "h_decoder=Dense(intermediate_dim, activation='relu')\n",
    "x_bar=Dense(original_dim,activation='sigmoid')\n",
    "h_decoded = h_decoder(z)\n",
    "x_decoded = x_bar(h_decoded)\n",
    "vae = Model(x, [x_decoded,z])\n",
    "\n",
    "weightsPath = \"./mov_genome.hdf5\"\n",
    "vae.load_weights(weightsPath)\n",
    "x_test_matrix = np.load( open( \"movie_genomes.npy\", \"rb\" ) )\n",
    "x_test_reconstructed = vae.predict(x_test_matrix, batch_size=batch_size)  # float values per user\n",
    "with open('genome_embed.npy', 'wb') as f:\n",
    "    np.save(f, np.array(x_test_reconstructed[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train regular user-movie VAE with movie feature embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "original_dim=n_movie\n",
    "intermediate_dim=80\n",
    "latent_dim=20\n",
    "nb_epochs=5 \n",
    "epsilon_std=1.0\n",
    "\n",
    "vocab_size = n_movie\n",
    "embed_dim = 3\n",
    "seq_length = n_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (10, 7345)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (10, 7345, 3)        22035       input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (10, 22035)          0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (10, 80)             1762880     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (10, 20)             1620        dense_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (10, 20)             1620        dense_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (10, 20)             0           dense_107[0][0]                  \n",
      "                                                                 dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (10, 80)             1680        lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (10, 7345)           594945      dense_109[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,384,780\n",
      "Trainable params: 2,384,780\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = x_test_reconstructed[1]\n",
    "# embedding_matrix = np.append(np.array([[0.0, 0.0, 0.0]]) ,embedding_matrix, axis =0)\n",
    "x=Input(batch_shape=(batch_size,original_dim))\n",
    "embedding_layer = Embedding(vocab_size, 3, weights=[embedding_matrix], input_length=seq_length, trainable=True)\n",
    "embed = embedding_layer(x)\n",
    "flat_embed = Flatten()\n",
    "embed = flat_embed(embed)\n",
    "h=Dense(intermediate_dim, activation='tanh')(embed)\n",
    "\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "def sampling(args):\n",
    "    _mean,_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return _mean+K.exp(_log_var/2)*epsilon\n",
    "z= Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# decoder network\n",
    "h_decoder=Dense(intermediate_dim, activation='tanh')\n",
    "x_bar=Dense(original_dim, activation='softmax')\n",
    "h_decoded = h_decoder(z)\n",
    "x_decoded = x_bar(h_decoded)\n",
    "\n",
    "# mul_inp = Input(batch_shape=(batch_size,original_dim))\n",
    "# x_decoded = keras.layers.Multiply()([x_decoded, mul_inp])\n",
    "# vae = Model([x, mul_inp], x_decoded)\n",
    "\n",
    "#x_decoded = merge([x_decoded, mul_inp], mode = 'mul')\n",
    "vae = Model(x, x_decoded)\n",
    "\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "print(vae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                             (rows, cols)), dtype='float64',\n",
    "                             shape=(n_users, n_movie))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = load_train_data('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_indices = np.array([range(1,n_movie+1)])\n",
    "movie_indices = np.repeat(movie_indices, batch_size, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_batch_generator(x, y, batch_size, samples_per_epoch):\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    x =  x[shuffle_index, :]\n",
    "    y =  y[shuffle_index, :]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        x_batch = np.array(x[index_batch,:].todense()).astype(float)\n",
    "        x_new_batch = x_batch*movie_indices\n",
    "        \n",
    "        counter += 1\n",
    "        yield (x_batch, x_batch)\n",
    "        if (counter >= number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., epochs=5, steps_per_epoch=1000)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 543.4325 4s -\n",
      "\n",
      "Epoch 00001: saving model to weights_h-vae_imdb.hdf5\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 535.9426\n",
      "\n",
      "Epoch 00002: saving model to weights_h-vae_imdb.hdf5\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 533.4157\n",
      "\n",
      "Epoch 00003: saving model to weights_h-vae_imdb.hdf5\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 532.2804\n",
      "\n",
      "Epoch 00004: saving model to weights_h-vae_imdb.hdf5\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 63s 63ms/step - loss: 531.9121\n",
      "\n",
      "Epoch 00005: saving model to weights_h-vae_imdb.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c5a8d48eb8>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsPath = \"weights_h-vae_imdb.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=weightsPath, verbose=1)\n",
    "\n",
    "vae.fit_generator(nn_batch_generator(x_train, x_train, batch_size, 1000) , samples_per_epoch=1000, nb_epoch=nb_epochs, callbacks = [checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use VAE to recreate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "    rte = pd.concat([rows_tr, rows_te], ignore_index=True)\n",
    "    cte = pd.concat([cols_tr, cols_te], ignore_index=True)\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_movie))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rte),\n",
    "                             (rte, cte)), dtype='float64', shape=(end_idx - start_idx + 1, n_movie))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te = load_tr_te_data('test_tr.csv','test_te.csv')\n",
    "test_matrix = np.squeeze(np.asarray(x_te.todense()))\n",
    "train_matrix= np.squeeze(np.asarray(x_tr.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_reconstructed = vae.predict(test_matrix, batch_size=batch_size)  # float values per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated_movies_idxs=[]\n",
    "for i in range(len(test_matrix)):\n",
    "    top_rated_movies_idxs.append([i for i, x in enumerate(test_matrix[i]) if x == 1.0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG at 100:  0.17503165510705818\n",
      "recall at 20:  0.29224579632398895\n",
      "recall at 50:  0.29288700258579703\n",
      "Catalog coverage@20: test 0.0050.\n",
      "Catalog coverage@50: test 0.0135.\n"
     ]
    }
   ],
   "source": [
    "def cov(x_test, x_test_reconstructed, k):\n",
    "    idxset = set()\n",
    "    for i in range(len(x_test)):\n",
    "        sorted_ratings = x_test_reconstructed[i].tolist()\n",
    "        top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "        idxset.update(top_predicted_movies_idx)\n",
    "    return len(idxset)/len(x_test_reconstructed[0])\n",
    "def recallatk(x_test, x_test_reconstructed, k):\n",
    "    recall_values = []\n",
    "    total_recall = 0.0\n",
    "    for i in range(len(x_test)):\n",
    "        \n",
    "        top_rated_movies_idx = top_rated_movies_idxs[i]\n",
    "        if len(top_rated_movies_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        sorted_ratings = x_test_reconstructed[i].tolist()\n",
    "        top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "        \n",
    "        sum = 0.0\n",
    "        for i in range(0, k):\n",
    "            if top_predicted_movies_idx[i] in top_rated_movies_idx:\n",
    "                sum+=1.0\n",
    "        recall = sum/float(min(k, len(top_rated_movies_idx)))\n",
    "        total_recall += recall\n",
    "        recall_values.append(recall)\n",
    "    return total_recall/float(len(recall_values))\n",
    "\n",
    "def ndcgatk(x_test, x_test_reconstructed, k):\n",
    "    ndcg_values = []\n",
    "    total_ndcg = 0.0\n",
    "    best  = 0.0\n",
    "    for i in range(len(x_test)):\n",
    "        top_rated_movies_idx = top_rated_movies_idxs[i]\n",
    "        if len(top_rated_movies_idx) == 0:\n",
    "            continue\n",
    "        sorted_ratings = x_test_reconstructed[i].tolist()\n",
    "        top_predicted_movies_idx = sorted(range(len(sorted_ratings)), key=lambda i: sorted_ratings[i])[-k:]\n",
    "        sum_ndcg = 0\n",
    "        for i in range(0, k):\n",
    "            if top_predicted_movies_idx[i] in top_rated_movies_idx:\n",
    "                ndcg = 1/(math.log(i+2))\n",
    "            else:\n",
    "                ndcg = 0\n",
    "            sum_ndcg += ndcg\n",
    "\n",
    "        total_ndcg += sum_ndcg\n",
    "        ndcg_values.append(sum_ndcg)\n",
    "\n",
    "    ndcg_values = np.array(ndcg_values)\n",
    "    max_ndcg = ndcg_values.max()\n",
    "    ndcg_values = ndcg_values / max_ndcg \n",
    "    total_ndcg = np.sum(ndcg_values)\n",
    "\n",
    "    return total_ndcg/float(len(ndcg_values))\n",
    "\n",
    "\n",
    "\n",
    "print(\"NDCG at 100: \", ndcgatk(test_matrix, x_test_reconstructed, 100))\n",
    "print(\"recall at 20: \", recallatk(test_matrix, x_test_reconstructed, 20))\n",
    "print(\"recall at 50: \", recallatk(test_matrix, x_test_reconstructed, 50))\n",
    "print ('Catalog coverage@20: test %.4f.' % (cov(test_matrix, x_test_reconstructed, 20)))\n",
    "print ('Catalog coverage@50: test %.4f.' % (cov(test_matrix, x_test_reconstructed, 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE without Genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "history = LossHistory()\n",
    "\n",
    "\n",
    "\n",
    "batch_size=10\n",
    "original_dim=n_movie\n",
    "intermediate_dim=80\n",
    "latent_dim=20\n",
    "nb_epochs=10 \n",
    "epsilon_std=1.0\n",
    "\n",
    "\n",
    "# encoder network\n",
    "x=Input(batch_shape=(batch_size,original_dim))\n",
    "h=Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "# sampling from latent dimension for decoder/generative part of network\n",
    "def sampling(args):\n",
    "    _mean,_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return _mean+K.exp(_log_var/2)*epsilon\n",
    "\n",
    "z= Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# decoder network\n",
    "h_decoder=Dense(intermediate_dim, activation='tanh')\n",
    "x_bar=Dense(original_dim,activation='softmax') # this should be softmax right?\n",
    "h_decoded = h_decoder(z)\n",
    "x_decoded = x_bar(h_decoded)\n",
    "\n",
    "# build and compile model\n",
    "vae = Model(x, x_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training users:  118287\n"
     ]
    }
   ],
   "source": [
    "def vae_loss(x,x_bar):\n",
    "    reconst_loss=original_dim*objectives.binary_crossentropy(x, x_bar)\n",
    "    kl_loss= -0.5*K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return kl_loss + reconst_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "\n",
    "x_train = load_train_data('train.csv')\n",
    "print(\"number of training users: \", x_train.shape[0])\n",
    "\n",
    "x_val = load_train_data('validation_tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., epochs=10, steps_per_epoch=2500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 83s 33ms/step - loss: 505.6696\n",
      "Epoch 2/10\n",
      "   3/2500 [..............................] - ETA: 1:04 - loss: 406.9386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:435: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\EJ\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:958: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 77s 31ms/step - loss: 478.8107\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 76s 31ms/step - loss: 467.9488\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 460.3736\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 76s 30ms/step - loss: 454.6521\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 78s 31ms/step - loss: 449.8336\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 445.9990 1s - loss: 444. - ETA: 0s -\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 442.9193\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 77s 31ms/step - loss: 440.4328\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 78s 31ms/step - loss: 438.4467\n",
      "training losses over epochs\n",
      "[505.66963980102537, 478.8106668701172, 467.9487526306152, 460.37361370239256, 454.6521313659668, 449.8336029968262, 445.9989737792969, 442.9192685852051, 440.432764654541, 438.44670670776367]\n"
     ]
    }
   ],
   "source": [
    "def nn_batch_generator(x, y, batch_size, samples_per_epoch):\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    x =  x[shuffle_index, :]\n",
    "    y =  y[shuffle_index, :]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        x_batch = x[index_batch,:].todense()\n",
    "        y_batch = y[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield (np.array(x_batch),np.array(y_batch))\n",
    "        if (counter >= number_of_batches):\n",
    "            counter=0\n",
    "\n",
    "\n",
    "weightsPath = \"./tmp/weights.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=weightsPath, verbose=1, save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# sending complete training data and shuffle flag will shuffle so that each user comes atleast once in training because of multiple epochs\n",
    "vae.fit_generator(nn_batch_generator(x_train, x_train, batch_size, 2500), samples_per_epoch=2500 ,nb_epoch=nb_epochs, callbacks=[checkpointer, reduce_lr, history])\n",
    "\n",
    "print(\"training losses over epochs\")\n",
    "print(history.losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG at 100:  0.2596440767791029\n",
      "recall at 20:  0.4529321981995462\n",
      "recall at 50:  0.4866546361681343\n",
      "Catalog coverage@20: test 0.4543.\n",
      "Catalog coverage@50: test 0.5532.\n"
     ]
    }
   ],
   "source": [
    "x_test_reconstructed = vae.predict(test_matrix, batch_size=batch_size)  # float values per user\n",
    "print(\"NDCG at 100: \", ndcgatk(test_matrix, x_test_reconstructed, 100))\n",
    "print(\"recall at 20: \", recallatk(test_matrix, x_test_reconstructed, 20))\n",
    "print(\"recall at 50: \", recallatk(test_matrix, x_test_reconstructed, 50))\n",
    "print ('Catalog coverage@20: test %.4f.' % (cov(test_matrix, x_test_reconstructed, 20)))\n",
    "print ('Catalog coverage@50: test %.4f.' % (cov(test_matrix, x_test_reconstructed, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "i1H4a2TC7kWS",
    "IFyg3xSn7kWZ",
    "uMBzfzyz7kXB",
    "_JVgbeSp7kXP"
   ],
   "name": "VAE_ML20M_WWW2018.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
